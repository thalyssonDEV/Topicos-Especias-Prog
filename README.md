# Atividade Pr√°tica: Tuning de Hiperpar√¢metros e Compara√ß√£o de Modelos

Projeto desenvolvido para a disciplina de **T√≥picos Especiais em Programa√ß√£o** do curso de An√°lise e Desenvolvimento de Sistemas do IFPI.

## üéØ Objetivo

O objetivo deste projeto √© aplicar e comparar t√©cnicas de ajuste de hiperpar√¢metros (Tuning) em diferentes modelos de classifica√ß√£o supervisionada. A an√°lise compara o desempenho e o custo computacional de cada t√©cnica.

* **Modelos Utilizados**: `LogisticRegression`, `SVC`, `RandomForestClassifier`
* **Bases de Dados**: `load_wine()`, `load_digits()`, `load_breast_cancer()`
* **T√©cnicas de Tuning**: `GridSearchCV`, `RandomizedSearchCV`, `BayesSearchCV`

## üõ†Ô∏è Requisitos

O script utiliza Python 3. As bibliotecas necess√°rias podem ser instaladas via `pip`:

```bash
pip install pandas scikit-learn scikit-optimize tabulate
```

## üöÄ Execu√ß√£o

Para executar os testes e imprimir as tabelas de resultados no console, execute o script principal:
```bash
python3 main.py
```

## üìä An√°lise Conclusiva

A an√°lise dos resultados indica que n√£o h√° um "melhor modelo" universal; o desempenho dependeu do dataset. O SVM foi superior no 'Digits' (Acur√°cia 0.9566), enquanto a Regress√£o Log√≠stica e o SVM se destacaram nos datasets 'Wine' e 'Breast Cancer'. A t√©cnica de tuning mais eficiente foi o RandomizedSearchCV, que entregou m√©tricas de performance quase id√™nticas ao GridSearchCV (ex: 0.9560 no 'Digits, SVM'), por√©m com um custo computacional muito menor (11.80s vs 24.86s). Observou-se que os datasets 'Wine' e 'Breast Cancer' foram simples, com os modelos baseline j√° atingindo acur√°cias de ~98%, tornando o tuning quase irrelevante. Em contraste, o 'Digits' foi o mais complexo, onde o tuning se mostrou mais impactante, elevando a performance dos modelos.

## Tabelas Comparativas

![Imagem1](caminho/para/imagem.png)
![Imagem2](caminho/para/imagem2.png)
